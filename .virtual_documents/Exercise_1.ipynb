import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import train_test_split








# loading the data
df = pd.read_csv('data.csv')
df.head()


#looking for null data
df.isna().sum()


df.drop(axis=1,columns='Unnamed: 32',inplace=True)
df.isna().sum()


# our target is diagnosis, so we divede our data in X,y X: input space , y is the target
X , y = df.iloc[:,2:],df.iloc[:,1]


# getting the index of the two major mutual information between two features and the target
indices_maximos = np.argsort(mutual_info_classif(X,y))[-2:][::-1]
data = X.iloc[:,indices_maximos]
data.loc[:,'target'] = y
sb.scatterplot(data=data,x='perimeter_worst',y='area_worst',hue='target')


data.target.value_counts()





# filter and picking 50 samples for each label of the target 

data_filtrado_1 = data[data["perimeter_worst"] < 110].sample(n=50, random_state=2)  
data_filtrado_2 = data[data["area_worst"] > 1200].sample(n=50, random_state=2)  

# join the two filters
data_sample = pd.concat([data_filtrado_1, data_filtrado_2]).reset_index(drop=True)

sb.scatterplot(data=data_sample,x='perimeter_worst',y='area_worst',hue='target')


data_sample.target.value_counts()


X , y = data_sample.iloc[:,:-1], data_sample.iloc[:,-1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12,stratify=y)


train_counts = y_train.value_counts()
test_counts = y_test.value_counts()

y_max = max(train_counts.max(), test_counts.max())
fig, axes = plt.subplots(1, 2, figsize=(6, 3))

# Plotting the value counts of y_train
sb.barplot(x=train_counts.index, y=train_counts.values, ax=axes[0],hue=train_counts.index)
axes[0].set_title("Training")
axes[0].set_xlabel("Clases")
axes[0].set_ylabel("Frecuencia")
axes[0].set_ylim(0, y_max)  

# Plotting the value counts of y_test
sb.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1],hue=test_counts.index)
axes[1].set_title("Test")
axes[1].set_xlabel("Clases")
axes[1].set_ylabel("Frecuencia")
axes[1].set_ylim(0, y_max)  

plt.tight_layout()
plt.show()


y_train.loc[y_train=='M']=1.0
y_train.loc[y_train=='B']=0.0
y_test.loc[y_test=='M']=1.0
y_test.loc[y_test=='B']=0.0


y_train.value_counts()


y_test.value_counts()


X_train = np.array(X_train, dtype=np.float32)
X_test = np.array(X_test, dtype=np.float32)
y_train = np.array(y_train, dtype=np.float32)
y_test = np.array(y_test, dtype=np.float32)


X_train.shape,X_test.shape,y_train.shape,y_test.shape


def scaler(x):
    return (x - np.mean(x)) / np.std(x)


X_train = scaler(X_train)
X_test = scaler(X_test)





from tensorflow import keras


from keras.layers import  Dense
from keras.models import Sequential
from keras.optimizers import Adam


# the percertron arquitecture 
model = Sequential()
model.add(Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='glorot_uniform'))
model.compile(loss='MSE', optimizer=Adam(), metrics=['MSE'])


# training
history = model.fit(X_train, y_train.reshape(-1,1), epochs=100, batch_size=5, verbose=0, validation_split=0.2)


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'], loc='upper left')
plt.show()





_, accuracy = model.evaluate(X_test, y_test.reshape(-1,1),verbose=False)
print("%0.3f" % accuracy)


y_pred = model.predict(X_test)


y_pred_bin = (y_pred >= 0.5).astype(int).flatten()


from sklearn.metrics import confusion_matrix , accuracy_score


CM = confusion_matrix(y_pred_bin,y_test)
CM


accuracy_score(y_test,y_pred_bin)


sb.scatterplot(x=X_test[:,0],y=X_test[:,1],hue=y_test)


for i, w in enumerate(model.get_weights()):
    print(f"Pesos de la capa {i}:\n", w)


# Pesos del perceptrón
W1 = 0.2569969
W2 = 1.5319537
b = -0.5779063

# Rango de valores para x1
x1 = np.linspace(-3, 3, 100)

# Cálculo de x2 en base a la ecuación de la recta
x2 = - (W1 / W2) * x1 - (b / W2)

# Graficar la recta
plt.plot(x1, x2, label="Frontera de decisión", color='red')

# Formato de la gráfica
sb.scatterplot(x=X_test[:,0],y=X_test[:,1],hue=y_test)
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.xlim(-2, 0)
plt.ylim(-2, 5)
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.legend()
plt.grid()
plt.title("Frontera de decisión del perceptrón")

# Mostrar la gráfica
plt.show()
